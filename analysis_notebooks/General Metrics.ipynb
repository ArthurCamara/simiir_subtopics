{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d46aeb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T22:20:38.386936Z",
     "start_time": "2021-10-07T22:20:38.348033Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from re import finditer\n",
    "from itertools import tee\n",
    "import pickle\n",
    "import urllib\n",
    "from collections.abc import Iterable, Iterator\n",
    "from itertools import product\n",
    "\n",
    "### Utils\n",
    "\n",
    "def g_path(*argv: str)->str:\n",
    "    \"\"\"short hand for creating a new path properly\n",
    "    args:\n",
    "        argv: vector of strings to join into a path\"\"\"\n",
    "    return os.path.join(*argv)\n",
    "\n",
    "def pairwise(iterable:Iterable)->Iterator:\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "#Mappings\n",
    "topics = {\n",
    "    \"ethics\": 1,\n",
    "    \"genetically_modified_organism\": 2,\n",
    "    \"noise-induced_hearing_loss\": 3,\n",
    "    \"subprime_mortgage_crisis\": 4,\n",
    "    \"radiocarbon_dating_considerations\": 5,\n",
    "    \"business_cycle\": 7,\n",
    "    \"irritable_bowel_syndrome\": 8,\n",
    "    \"theory_of_mind\": 9\n",
    "}\n",
    "\n",
    "subtopic_strategies = {\n",
    "    \"Greedy\": 1,\n",
    "    \"Random\": 2,\n",
    "    \"Reverse\": 3,\n",
    "    \"GreedySmart\": 4\n",
    "}\n",
    "vocab = json.load(open(\"../CHIIR21-SAL-Scaffolding/data/vocab.json\"))\n",
    "topics = {x['title']:k for k,x in vocab.items() if x['title'] != \"Sports\" and x['title']!=\"Norepinephrine\" and x['title']!=\"Research in lithium-ion batteries\"}\n",
    "\n",
    "subtopics = json.load(open(\"../CHIIR21-SAL-Scaffolding/data/topics.json\"))\n",
    "subtopics = {x['title'] :x['terms'] for x in subtopics.values() if x['title'] != \"Sports\" and x['title']!=\"Norepinephrine\" and x['title']!=\"Research in lithium-ion batteries\"}\n",
    "subtopics[\"Ethics\"][\"Meta-ethics\"] = subtopics[\"Ethics\"].pop(\"Meta ethics\")\n",
    "\n",
    "subtopics ={k: list(map(lambda x: urllib.parse.quote(x) , v.keys())) for k, v in subtopics.items()}  # Dict[topic_name, List[subtopic]] , Dict[str, List[str]]\n",
    "subtopics_keywords = pickle.load(open(\"../data/subtopic_l2_keywords.pkl\", 'rb'))  # Dict[subtopic, Set[keywords]], Dict[str, Set[str]]\n",
    "\n",
    "methods = subtopic_strategies.keys()\n",
    "\n",
    "params = {\"lambda\": [0.1, 0.4, 0.8],\n",
    "          \"limit\": [2.0, 6.0, 10.0],\n",
    "          \"threshold\":[0.0, 1.0, 3.0, 5.0]\n",
    "         }\n",
    "\n",
    "all_users = list(product(params['limit'], params['threshold'], params['lambda']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fd36e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T22:21:09.710235Z",
     "start_time": "2021-10-07T22:21:09.689928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS LONG AF\n",
    "HOME_PATH = \"data/simulation_data\"\n",
    "\n",
    "\n",
    "def get_file_name(subtopic_strategy, limit, sn_threshold, sn_lambda,doc_lambda, topic, extension, run_id):\n",
    "    limit = f\"{str(limit).replace('.', '')}\"\n",
    "    sn_threshold = f\"{str(sn_threshold).replace('.', '')}\"\n",
    "    sn_lambda = f\"{str(sn_lambda).replace('.', '')}\"\n",
    "    doc_lambda = f\"{str(doc_lambda).replace('.', '')}\"\n",
    "\n",
    "    partial_filename = f\"run_{run_id}_sub{subtopic_strategy}-l{limit}_q13_ss1_se0_sn0-t{sn_threshold}l{sn_lambda}_doc0-l{doc_lambda}\"\n",
    "    run_file_template = f\"{HOME_PATH}/run_{run_id}_sub{subtopic_strategy}/vars-l{limit}/q13/ss1/se0/sn0/vars-t{sn_threshold}l{sn_lambda}/doc0/vars-l{doc_lambda}/output/{partial_filename}-{topic}-user-{partial_filename}.{extension}\"\n",
    "\n",
    "    return run_file_template\n",
    "\n",
    "\n",
    "a = get_file_name(1, 10.0, 5.0, 0.8, 0.1, \"ethics\", \"log\",1)\n",
    "os.path.isfile(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0a4dde3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T09:01:01.244650Z",
     "start_time": "2021-10-08T09:00:58.750607Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "query_per_method = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "docs_per_method = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "snippets_per_method = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "\n",
    "query_per_user = defaultdict(lambda:[])\n",
    "docs_per_user = defaultdict(lambda:[])\n",
    "snippets_per_user = defaultdict(lambda:[])\n",
    "\n",
    "\n",
    "for (limit, threshold, lam) in all_users:\n",
    "    u_id = (limit, threshold, lam)\n",
    "    for method in methods:\n",
    "        method_id = subtopic_strategies[method]\n",
    "        for topic in topics:\n",
    "            n_queries = []\n",
    "            n_snippets = []\n",
    "            n_docs = []\n",
    "            for r in range(10):\n",
    "                c_topic = topic.lower().replace(\" \", \"_\")\n",
    "                log_file = get_file_name(method_id, limit, threshold, lam, 0.1, c_topic, \"log\", r)\n",
    "                lines = open(log_file).readlines()\n",
    "                n_queries.append(int(open(log_file).readlines()[-6].strip().split()[-1]))\n",
    "                n_snippets.append(int(open(log_file).readlines()[-5].strip().split()[-1]))\n",
    "                n_docs.append(int(open(log_file).readlines()[-3].strip().split()[-1]))\n",
    "            query_per_method[method][u_id].append(np.mean(n_queries))\n",
    "            docs_per_method[method][u_id].append(np.mean(n_docs))\n",
    "            snippets_per_method[method][u_id].append(np.mean(n_snippets))\n",
    "            \n",
    "            query_per_user[u_id].append(np.mean(n_queries))\n",
    "            docs_per_user[u_id].append(np.mean(n_docs))\n",
    "            snippets_per_user[u_id].append(np.mean(n_snippets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
