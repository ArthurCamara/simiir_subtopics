{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T14:05:30.718878Z",
     "iopub.status.busy": "2022-01-17T14:05:30.718435Z",
     "iopub.status.idle": "2022-01-17T14:05:30.730877Z",
     "shell.execute_reply": "2022-01-17T14:05:30.729912Z",
     "shell.execute_reply.started": "2022-01-17T14:05:30.718822Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from re import finditer\n",
    "from itertools import tee\n",
    "import pickle\n",
    "import urllib\n",
    "from collections.abc import Iterable, Iterator\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "### Utils\n",
    "\n",
    "def g_path(*argv: str)->str:\n",
    "    \"\"\"short hand for creating a new path properly\n",
    "    args:\n",
    "        argv: vector of strings to join into a path\"\"\"\n",
    "    return os.path.join(*argv)\n",
    "\n",
    "def pairwise(iterable:Iterable)->Iterator:\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:04:11.741000Z",
     "start_time": "2021-10-04T09:04:09.162796Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-17T14:05:31.372118Z",
     "iopub.status.busy": "2022-01-17T14:05:31.371765Z",
     "iopub.status.idle": "2022-01-17T14:05:31.386320Z",
     "shell.execute_reply": "2022-01-17T14:05:31.385475Z",
     "shell.execute_reply.started": "2022-01-17T14:05:31.372073Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation_output_folder = \"../simulations/\"\n",
    "\n",
    "vocab = json.load(open(\"data/vocab.json\"))\n",
    "topics = {x['title']:k for k,x in vocab.items() if x['title'] != \"Sports\" and x['title']!=\"Norepinephrine\" and x['title']!=\"Research in lithium-ion batteries\"}\n",
    "\n",
    "subtopics = json.load(open(\"data/topics.json\"))\n",
    "subtopics = {x['title'] :x['terms'] for x in subtopics.values() if x['title'] != \"Sports\" and x['title']!=\"Norepinephrine\" and x['title']!=\"Research in lithium-ion batteries\"}\n",
    "subtopics[\"Ethics\"][\"Meta-ethics\"] = subtopics[\"Ethics\"].pop(\"Meta ethics\")\n",
    "\n",
    "subtopics ={k: list(map(lambda x: urllib.parse.quote(x) , v.keys())) for k, v in subtopics.items()}  # Dict[topic_name, List[subtopic]] , Dict[str, List[str]]\n",
    "subtopics_keywords = pickle.load(open(\"data/subtopic_l2_keywords.pkl\", 'rb'))  # Dict[subtopic, Set[keywords]], Dict[str, Set[str]]\n",
    "\n",
    "methods = {x.split(\"_\")[0] for x in os.listdir(simulation_output_folder) if x!=\"COMPLETED\"}\n",
    "\n",
    "params = {\"lambda\": [0.1, 0.4, 0.8],\n",
    "          \"limit\": [2.0, 6.0, 10.0],\n",
    "          \"threshold\":[0.0, 1.0, 3.0, 5.0]\n",
    "         }\n",
    "\n",
    "all_users = list(product(params['limit'], params['threshold'], params['lambda']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:04:11.747036Z",
     "start_time": "2021-10-04T09:04:11.742765Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-17T14:05:32.884590Z",
     "iopub.status.busy": "2022-01-17T14:05:32.884162Z",
     "iopub.status.idle": "2022-01-17T14:05:32.891803Z",
     "shell.execute_reply": "2022-01-17T14:05:32.890722Z",
     "shell.execute_reply.started": "2022-01-17T14:05:32.884536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Mappings\n",
    "topics = {\n",
    "    \"ethics\": 1,\n",
    "    \"genetically_modified_organism\": 2,\n",
    "    \"noise-induced_hearing_loss\": 3,\n",
    "    \"subprime_mortgage_crisis\": 4,\n",
    "    \"radiocarbon_dating_considerations\": 5,\n",
    "    \"business_cycle\": 7,\n",
    "    \"irritable_bowel_syndrome\": 8,\n",
    "    \"theory_of_mind\": 9\n",
    "}\n",
    "\n",
    "subtopic_strategies = {\n",
    "    \"Greedy\": 1,\n",
    "    \"Random\": 2,\n",
    "    \"Reverse\": 3,\n",
    "    \"GreedySmart\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:04:11.767446Z",
     "start_time": "2021-10-04T09:04:11.748621Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-17T14:05:34.995092Z",
     "iopub.status.busy": "2022-01-17T14:05:34.994670Z",
     "iopub.status.idle": "2022-01-17T14:05:35.015325Z",
     "shell.execute_reply": "2022-01-17T14:05:35.014588Z",
     "shell.execute_reply.started": "2022-01-17T14:05:34.995038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS LONG AF\n",
    "HOME_PATH = \"data/simulation_data\"\n",
    "\n",
    "\n",
    "def get_file_name(subtopic_strategy, limit, sn_threshold, sn_lambda,doc_lambda, topic, extension, run_id):\n",
    "    limit = f\"{str(limit).replace('.', '')}\"\n",
    "    sn_threshold = f\"{str(sn_threshold).replace('.', '')}\"\n",
    "    sn_lambda = f\"{str(sn_lambda).replace('.', '')}\"\n",
    "    doc_lambda = f\"{str(doc_lambda).replace('.', '')}\"\n",
    "\n",
    "    partial_filename = f\"run_{run_id}_sub{subtopic_strategy}-l{limit}_q13_ss1_se0_sn0-t{sn_threshold}l{sn_lambda}_doc0-l{doc_lambda}\"\n",
    "    run_file_template = f\"{HOME_PATH}/run_{run_id}_sub{subtopic_strategy}/vars-l{limit}/q13/ss1/se0/sn0/vars-t{sn_threshold}l{sn_lambda}/doc0/vars-l{doc_lambda}/output/{partial_filename}-{topic}-user-{partial_filename}.{extension}\"\n",
    "\n",
    "    return run_file_template\n",
    "\n",
    "\n",
    "a = get_file_name(1, 10.0, 5.0, 0.8, 0.1, \"ethics\", \"log\",1)\n",
    "os.path.isfile(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T14:06:00.869342Z",
     "iopub.status.busy": "2022-01-17T14:06:00.868928Z",
     "iopub.status.idle": "2022-01-17T14:06:00.878312Z",
     "shell.execute_reply": "2022-01-17T14:06:00.876990Z",
     "shell.execute_reply.started": "2022-01-17T14:06:00.869288Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take the weight of each topic, according to the number of subtopics\n",
    "number_of_keywords_per_topic = dict()\n",
    "for topic in subtopics.keys():\n",
    "    encoded_topic_name = urllib.parse.quote(topic)\n",
    "    topic_subtopics = [\"/\".join((encoded_topic_name, x)) for x in subtopics[topic]]\n",
    "    all_keywords = set.union(*[subtopics_keywords[v] for v in topic_subtopics if v in subtopics_keywords])\n",
    "    number_of_keywords_per_topic[topic] = len(all_keywords)\n",
    "\n",
    "max_subtopic_keywords = max(number_of_keywords_per_topic.values())\n",
    "normalization_factors = {k:v/max_subtopic_keywords for (k, v) in number_of_keywords_per_topic.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T14:06:02.221832Z",
     "iopub.status.busy": "2022-01-17T14:06:02.221423Z",
     "iopub.status.idle": "2022-01-17T14:06:02.226702Z",
     "shell.execute_reply": "2022-01-17T14:06:02.225540Z",
     "shell.execute_reply.started": "2022-01-17T14:06:02.221779Z"
    }
   },
   "outputs": [],
   "source": [
    "local_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-17T14:06:03.833507Z",
     "iopub.status.busy": "2022-01-17T14:06:03.833061Z",
     "iopub.status.idle": "2022-01-17T14:06:04.453870Z",
     "shell.execute_reply": "2022-01-17T14:06:04.453278Z",
     "shell.execute_reply.started": "2022-01-17T14:06:03.833454Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "pattern = re.compile(\"([^\\s\\w]|_)+\")\n",
    "\n",
    "\n",
    "import redis\n",
    "db_url = redis.Redis(db=10, decode_responses=True)\n",
    "def get_document_content(doc_url):\n",
    "    if doc_url in db_url:\n",
    "        return db_url.get(doc_url)\n",
    "    return \"\"\n",
    "\n",
    "def get_doc_keywords(doc, keywords):\n",
    "    doc = get_document_content(doc)\n",
    "    clean_doc = pattern.sub(\" \", doc)\n",
    "    clean_doc = [stemmer.stem(w.lower()) for w in clean_doc.split() if (w not in stop_words and stemmer.stem(w) in keywords)]\n",
    "    return Counter(clean_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T09:06:28.587316Z",
     "start_time": "2021-10-13T09:06:28.570117Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-17T14:06:05.081431Z",
     "iopub.status.busy": "2022-01-17T14:06:05.081057Z",
     "iopub.status.idle": "2022-01-17T14:06:05.097504Z",
     "shell.execute_reply": "2022-01-17T14:06:05.096867Z",
     "shell.execute_reply.started": "2022-01-17T14:06:05.081383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def completed_per_doc(file_name, keywords, topic, limit):\n",
    "    completed_pct_per_doc = []\n",
    "    for idx, line in enumerate(open(file_name)):\n",
    "        tracker = json.loads(line)\n",
    "        completed = sum([1 for k,v in tracker.items() if v >limit])/len(subtopics[topic])\n",
    "        completed_pct_per_doc.append(completed)\n",
    "    return completed_pct_per_doc\n",
    "\n",
    "\n",
    "def completed_subtopics_per_agent(limit, lam, threshold, topic, method,  n_runs):\n",
    "    method_id = subtopic_strategies[method]\n",
    "    c_topic =  topic.lower().replace(\" \", \"_\")\n",
    "    encoded_topic_name = urllib.parse.quote(topic)\n",
    "    topic_subtopics = [\"/\".join((encoded_topic_name, x)) for x in subtopics[topic]]\n",
    "    all_keywords = set.union(*[subtopics_keywords[v] for v in topic_subtopics if v in subtopics_keywords])\n",
    "    all_runs = []\n",
    "    for r in range(n_runs):\n",
    "        file_name = get_file_name(method_id, limit, threshold, lam, doc_lambda, c_topic, \"subtopics\", r)\n",
    "        all_runs.append(completed_per_doc(file_name, all_keywords, topic, limit))\n",
    "    \n",
    "    # pad if needed\n",
    "    longest_value = max([len(x) for x in all_runs])\n",
    "    for i in range(n_runs):\n",
    "        if len(all_runs[i]) < longest_value:\n",
    "            last_value = all_runs[i][-1]\n",
    "            all_runs[i].extend([last_value] * (longest_value - len(all_runs[i])))\n",
    "    # get the mean over all runs\n",
    "    mean_ys = []\n",
    "    for i in range(max([len(x) for x in all_runs])):\n",
    "        mean_ys.append(np.mean([x[i] for x in all_runs]))\n",
    "    return mean_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T20:23:32.059617Z",
     "start_time": "2021-10-07T20:23:02.702101Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-17T14:11:20.593291Z",
     "iopub.status.busy": "2022-01-17T14:11:20.592780Z",
     "iopub.status.idle": "2022-01-17T14:11:45.101202Z",
     "shell.execute_reply": "2022-01-17T14:11:45.100553Z",
     "shell.execute_reply.started": "2022-01-17T14:11:20.593234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f29e98e60b4e4d9c2fc76a143b5c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# %matplotlib inline\n",
    "subtopic = \"GLOBAL\"\n",
    "\n",
    "colors = {\"Greedy\": \"blue\", \"Random\": \"orange\", \"Reverse\": \"green\", \"GreedySmart\": \"red\"}\n",
    "    \n",
    "doc_lambda= 0.1 #FIXED\n",
    "\n",
    "\n",
    "min_count = 10\n",
    "\n",
    "\n",
    "for idx, (limit, threshold, lam) in tqdm(enumerate(all_users), total=27):\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(f\"plots/Subtopics_coverage/Subtopics_coverage-Limit-{limit}_Lambda-{lam}_Threshold-{threshold}.pdf\")\n",
    "    all_ys = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "    for topic in subtopics.keys():\n",
    "        fig = figure(figsize=(8, 6))\n",
    "        encoded_topic_name = urllib.parse.quote(topic)\n",
    "        for method in subtopic_strategies.keys():\n",
    "            y_values = completed_subtopics_per_agent(limit, lam, threshold, topic, method, 10)\n",
    "            x_values = list(range(len(y_values)))\n",
    "            all_ys[method][topic]=y_values\n",
    "            plt.plot(x_values, y_values, label=\" \".join(camel_case_split(method))+f\"({len(y_values)-1})\", c=colors[method])\n",
    "            \n",
    "        plt.xlabel(f\"Query number\", fontsize=20)\n",
    "        plt.title(topic, fontsize=15)\n",
    "        plt.ylabel(\"Fraction of explored subtopics\", fontsize=20)\n",
    "        plt.legend(fontsize=12)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close()\n",
    "    #Create the mean plots\n",
    "    fig = figure(figsize=(8, 6))\n",
    "    for method in all_ys:            \n",
    "        longest_session = max(map(len, all_ys[method].values()))\n",
    "        for topic in all_ys[method]:\n",
    "            try:\n",
    "                padding_needed = longest_session - len(all_ys[method][topic])\n",
    "                padding = [all_ys[method][topic][-1]] * padding_needed\n",
    "                all_ys[method][topic] += padding\n",
    "            except:\n",
    "                continue    \n",
    "        # For this method, for each query number (longest_session), multiply by the respective normalization factor and take the mean\n",
    "        mean_ys = []\n",
    "        for i in range(longest_session):\n",
    "            session_ys = []\n",
    "            for topic in all_ys[method]:\n",
    "                try:\n",
    "                    session_ys.append(all_ys[method][topic][i]*normalization_factors[topic])\n",
    "                except:\n",
    "                    continue\n",
    "            mean_ys.append(np.mean(session_ys))\n",
    "        plt.plot(range(longest_session), mean_ys, label=\" \".join(camel_case_split(method))+f\"({len(mean_ys)})\", c=colors[method])\n",
    "    plt.xlabel(f\"Doc number\", fontsize=20)\n",
    "    plt.title(f\"Mean - limit: {limit} threshold: {threshold} lambda:{lam}\", fontsize=15)\n",
    "    plt.ylabel(\"Fraction of explored subtopics\", fontsize=20)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlim(0, 30)\n",
    "    plt.ylim(0, 0.7)\n",
    "    pdf.savefig(fig)\n",
    "    plt.savefig(f\"plots/raw/subtopics/Limit-{limit}_Lambda-{lam}_Threshold-{threshold}.svg\")\n",
    "#     plt.show()\n",
    "    plt.close() \n",
    "    pdf.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
